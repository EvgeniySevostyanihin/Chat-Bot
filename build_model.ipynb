{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "выбранных слов 35029 / 37298\n",
      "Было 16200, стало 15800, 0.9753086419753086\n"
     ]
    }
   ],
   "source": [
    "from preparing import batch2train_data, load_voc\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "data, Vocabulary = load_voc()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Подготовка данных, для использования в модели\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "inp, lenghts, target, mask, max_target_len = batch2train_data([random.choice(data) for _ in range(2)])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([19, 2])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([5, 2])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_target_len"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Encoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "EMBEDDING_SIZE = 128"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.embedding = embedding\n",
    "        self.gru = nn.GRU(EMBEDDING_SIZE, EMBEDDING_SIZE, num_layers=2,\n",
    "                          dropout=0.4, bidirectional=True) # будет двухнаправленный, тоесть складывает матрицы\n",
    "                                                        # с конца и с начала, так больше профита даётся\n",
    "\n",
    "    def forward(self, input_seq, length, hidden=None):\n",
    "\n",
    "        embed = embedding(input_seq)\n",
    "\n",
    "        packed = pack_padded_sequence(embed, length) # это запоковаем\n",
    "\n",
    "        output, hidden = self.gru(packed, hidden)\n",
    "\n",
    "        output, _ = pad_packed_sequence(output) # это распаковывает\n",
    "\n",
    "        # здесь складываются 2 прохода (с конца и с начала)\n",
    "        output = output[:, :, EMBEDDING_SIZE:] + output[:, :, :EMBEDDING_SIZE]\n",
    "\n",
    "        return output, hidden\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(Vocabulary.num_words, EMBEDDING_SIZE)\n",
    "encoder = Encoder(embedding)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[ 0.3626, -0.1495, -0.2371,  ...,  0.2184, -0.1409, -0.1260],\n          [ 0.0742, -0.1199, -0.3195,  ..., -0.0956,  0.1990, -0.2521]],\n \n         [[ 0.6336, -0.3067, -0.4165,  ...,  0.3262, -0.1021, -0.1057],\n          [-0.1778, -0.0832, -0.1958,  ..., -0.0308, -0.0802, -0.3994]],\n \n         [[ 0.5875,  0.0475, -0.5112,  ...,  0.5937,  0.1572,  0.2276],\n          [-0.1604, -0.0012,  0.1265,  ..., -0.2252, -0.2972, -0.0621]],\n \n         ...,\n \n         [[ 0.4691,  0.0661, -0.1501,  ...,  0.4301, -0.0154,  0.1177],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n \n         [[ 0.4085,  0.2760, -0.0925,  ...,  0.5364, -0.1912,  0.2373],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n \n         [[ 0.0563,  0.0141, -0.1292,  ...,  0.2695, -0.2154,  0.0136],\n          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]],\n        grad_fn=<AddBackward0>),\n tensor([[[-0.3412, -0.0491,  0.2201,  ...,  0.1816,  0.5047,  0.2633],\n          [-0.3527, -0.3510,  0.0849,  ...,  0.0089,  0.4531,  0.2594]],\n \n         [[ 0.2890,  0.2281, -0.3251,  ...,  0.0567,  0.0222,  0.3713],\n          [ 0.5871,  0.3734, -0.0138,  ...,  0.2659, -0.5040,  0.3737]],\n \n         [[ 0.0572,  0.1457,  0.1049,  ...,  0.0949, -0.2445,  0.0257],\n          [ 0.1756, -0.0317,  0.3236,  ...,  0.2208, -0.2828,  0.1836]],\n \n         [[ 0.2546,  0.0081, -0.1562,  ..., -0.0189, -0.2289, -0.1524],\n          [-0.0189, -0.1453, -0.2697,  ..., -0.2554,  0.3406, -0.3083]]],\n        grad_fn=<StackBackward>))"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder(inp, lenghts)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "похоже работает"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-86459515",
   "language": "python",
   "display_name": "PyCharm (Emotion-Recognition)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}